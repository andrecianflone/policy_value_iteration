{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy_Value_Iteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrecianflone/policy_value_iteration/blob/master/Policy_Value_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-DWMKZnJRD1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Policy Iteration\n",
        "The state-value function for any given policy $\\pi$ can be evaluated as:\n",
        "$$\n",
        "\\newcommand{\\given}{|}\n",
        "\\newcommand{\\states}{S}\n",
        "\\newcommand{\\E}{\\mathbb{E}}\n",
        "\\newcommand{\\actions}{A}\n",
        "\\newcommand{\\argmax}{\\text{argmax}}\n",
        "\\newcommand{\\R}{\\mathbb{R}}\n",
        "V_\\pi (s) = \\sum_a \\pi(a\\given s) \\left[r(s,a) + \\gamma P(s^\\prime \\given s,a) V_\\pi (s^\\prime)\\right]\n",
        "$$\n",
        "\n",
        "for all $s \\in \\states$\n",
        "\n",
        "Equivalently, we can write the above as a function of state-action values:\n",
        "\n",
        "$$\n",
        "V_\\pi (s) = \\sum_a \\pi(a \\given s) q_\\pi(s,a)\n",
        "$$\n",
        "\n",
        "Which must be less than or equal to the max q-value:\n",
        "\n",
        "$$\n",
        "V_\\pi (s) = \\sum_a \\pi(a \\given s) q_\\pi(s,a) \\leq \\max_a q_\\pi(s,a)\n",
        "$$\n",
        "\n",
        "We perform a policy step improvement to get new policy $\\pi^\\prime$:\n",
        "\n",
        "$$\n",
        "\\pi^\\prime(a|s) = \\argmax_a q_\\pi (s,a)\n",
        "$$\n",
        "\n",
        "Then the following is true:\n",
        "\n",
        "$$\n",
        "V_\\pi (s) = \\sum_a \\pi(a \\given s) q_\\pi(s,a) \\leq \\sum_a \\pi^\\prime(a \\given s) q_\\pi(s,a)\n",
        "$$\n",
        "\n",
        "And if we improve again:\n",
        "\n",
        "$$\n",
        "\\pi^{\\prime\\prime} = (a|s) = \\argmax_a q_{\\pi^\\prime} (s,a)\n",
        "$$\n",
        "\n",
        "Then the following is also true:\n",
        "\n",
        "$$\n",
        "V_\\pi (s) = \\sum_a \\pi(a \\given s) q_\\pi(s,a) \\leq \\sum_a \\pi^\\prime(a \\given s) q_\\pi(s,a) \\leq \\sum_a \\pi^{\\prime\\prime}(a \\given s) q_{\\pi^\\prime}(s,a)\n",
        "$$\n",
        "\n",
        "And so on for all $s\\in \\states$. Since every application of policy iteration must result in a better policy, as evaluated by the value function, successive policies are nondecreasing. And, since the MDP is finite, therefore there is a finite set of policies, policy iteration *must* terminate. If such new policy results in state values equal to the states values under the previous policy, then the optimal policy must have been found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28vCF2l2yj4",
        "colab_type": "text"
      },
      "source": [
        "# Track 1: Policy Iteration and Value Iteration\n",
        "\n",
        "\n",
        "# Policy Iteration\n",
        "Policy iteration alternates between evaluating a policy and improving a policy. Policy evaluation is defined as the expected future reward, defined as the expectation for the infinite sum of discounted rewards:\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    v_\\pi(s) &= \\E_\\pi\\left[ \\sum_{t=0}^\\infty \\gamma^t r(S_t, A_t) \\given S_0 = s\\right] \\\\\n",
        "    &=\\E_\\pi\\left[ r(S_0, A_0) + \\sum_{t=1}^\\infty \\gamma^t r(S_t, A_t) \\given S_0 = s\\right] \\\\\n",
        "    &=\\E_\\pi\\left[ r(S_0, A_0) + \\sum_{t=0}^\\infty \\gamma^{t+1} r(S_{t+1}, A_{t+1}) \\given S_0 = s\\right] \\\\\n",
        "    &=\\E_\\pi\\left[ r(S_0, A_0) + \\gamma \\sum_{t=0}^\\infty \\gamma^{t} r(S_{t+1}, A_{t+1}) \\given S_0 = s\\right] \\\\    \n",
        "    &=\\E_\\pi\\left[ r(S_0, A_0) + \\gamma v_\\pi(S_1) \\given S_0 = s\\right] \\\\  \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Replacing the definition of the expectation with policy $\\pi$ and transition probability matrix $P$, we get:\n",
        "$$\n",
        "v_\\pi(s) = \\sum_{a \\in \\actions} \\pi\\left(a \\given s\\right)\\left( r(s, a) + \\gamma \\sum_{s'} P\\left(s' \\given s, a\\right) v_\\pi(s')\\right)\n",
        "$$\n",
        "\n",
        "Note this definition is slightly different, but equivalent to the notation in S&B, page 58:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "v_\\pi (s) &= \\E_\\pi \\left[ G_t \\given S_t = s\\right]\\\\\n",
        "&= \\E_\\pi \\left[ R_{t+1} + \\gamma G_{t+1} \\given S_t = s\\right]\\\\\n",
        "&= \\E_\\pi \\left[ R_{t+1} + \\gamma v_\\pi ( S_{t+1}) \\mid S_t = s \\right]\\\\\n",
        "&= \\sum_a \\pi(a|s) \\sum_{s^\\prime, r} p(s^\\prime, r \\mid s,a) \\left[r + \\gamma v_\\pi (s^\\prime)\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The pseudo code uses the latter notation, but we implement the algorithm with the former notation.\n",
        "\n",
        "We implement policy evaluation in matrix form. To do so, we define the following:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    r_\\pi(s) \\defeq \\sum_{a} \\pi\\left(a \\given s\\right) r(s,a) \\hspace{2em}P_\\pi(s, s') \\defeq \\sum_{a} \\pi\\left(a \\given s\\right) P\\left(s' \\given s, a\\right)\\enspace ,\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $r_\\pi \\in \\R^{|\\states|}$, $P_\\pi \\in \\R^{|\\states|\\times|\\states|}, v_\\pi \\in \\R^{\\states}$. In this form, $v_\\pi$ is defined as:\n",
        "$$\n",
        "v_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi\n",
        "$$\n",
        "\n",
        "Since we are solving for $v_\\pi$ iteratively, as opposed to closed form, we must keep track of our changing policy, and stationary reward functions. These are represented as matrices, where $\\pi(a \\given s) \\in \\R^{|\\states| \\times|\\actions|}$ and $r(s,a) \\in \\R^{|\\states| \\times |\\actions|}$.\n",
        "\n",
        "Pseudo code for policy iteration, which includes policy evaluation and improvement, from S&B p.63\n",
        "\n",
        "![policy iteration](https://image.ibb.co/daZKdm/policy_iteration.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2scNk1gGhhT",
        "colab_type": "text"
      },
      "source": [
        "## Policy and Value Iteration\n",
        "\n",
        "\n",
        "$$\n",
        "\\def\\E{\\mathbb{E}}\n",
        "\\def\\R{\\mathbb{R}}\n",
        "\\def\\given{\\mid}\n",
        "\\def\\actions{\\mathcal{A}}\n",
        "\\def\\states{\\mathcal{S}}\n",
        "\\def\\defeq{\\dot=}\n",
        "\\def\\argmax{\\text{argmax}}\n",
        "$$\n",
        "\n",
        "## Bellman Optimality Equation\n",
        "The Bellman Optimality Equation is defined as:\n",
        "\n",
        "$$\n",
        "V_* = \\max_a [r(s,a) + \\gamma \\sum_{s^\\prime} P(s^\\prime \\given s,a) V_* (s^\\prime)]\n",
        "$$\n",
        "\n",
        "For each step $k$, The Bellman Optimality Operator updates an estimate of $V_*$, that is $V_{k+1}$ based on previous estimate $V_{k}$:\n",
        "\n",
        "$$\n",
        "V_{k+1} = \\max_a [r(s,a) + \\gamma \\sum_{s^\\prime} P(s^\\prime \\given s,a) V_k (s^\\prime)]\n",
        "$$\n",
        "\n",
        "Let the error in our estimate be defined as the absolute distance between the estimate and the true value:\n",
        "\n",
        "$$\n",
        "\\epsilon_{k} = \\mid V_k - V_*|\n",
        "$$\n",
        "\n",
        "In the worst case, our error is upper bounded by the infinity norm:\n",
        "\n",
        "$$\n",
        "\\lVert V_k - V_* \\rVert_\\infty = \\max_s\\left| V_k (s) - V_*(s) \\right|\n",
        "$$\n",
        "\n",
        "Using these equations, let's apply the Bellman Optimality operator and show that with our error function it is a contraction operator:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "V_{k+1} - V_* &= \\left| \\max_a \\left[r(s,a) + \\gamma \\sum_{s^\\prime}P(s^\\prime \\given s,a)V_k(s^\\prime)\\right] - \\max_a \\left[r(s,a) + \\gamma \\sum_{s^\\prime}P(s^\\prime \\given s,a)V_*(s^\\prime)\\right] \\right| \\\\\n",
        "&\\leq \\max_a\\left|  r(s,a) + \\gamma \\sum_{s^\\prime}P(s^\\prime \\given s,a)V_k(s^\\prime) - r(s,a) + \\gamma \\sum_{s^\\prime}P(s^\\prime \\given s,a)V_*(s^\\prime)\\right| \\\\\n",
        "&= \\max_a\\left|\\gamma \\sum_{s^\\prime}P(s^\\prime \\given s,a)V_k(s^\\prime) -  \\gamma \\sum_{s^\\prime}P(s^\\prime \\given s,a)V_*(s^\\prime)\\right| \\\\\n",
        "&= \\gamma \\max_a\\left|\\sum_{s^\\prime}P(s^\\prime \\given s,a)\\left[V_k(s^\\prime) - V_*(s^\\prime)\\right]\\right| \\\\\n",
        "&\\leq \\gamma \\max_a\\left|\\max_{s}\\left[V_k(s) - V_*(s)\\right]\\right| \\\\\n",
        "&= \\gamma \\max_{s}\\left|V_k(s) - V_*(s)\\right| \\\\\n",
        "&= \\gamma \\lVert V_k - V_* \\rVert_\\infty\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Since applying one iteration of the operator must result in a decreasing error for all states $s \\in \\states$, with error equal to zero in the limit, the Bellman optimality operator is a contraction mapping.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISvuHhgPYy2U",
        "colab_type": "text"
      },
      "source": [
        "# Value Iteration\n",
        "One downside to policy iteration, which is clear from the pseudo code, is that after every stage of policy improvement we must re-evaluate our policy in a full sweep. We can combine policy improvement and evaluation into *value iteration*. For all $s \\in \\states$, value iteration is defined by the update rule:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "v_{k+1}(s) \\defeq &\\max_a \\E \\left[R_{t+1} + \\gamma v_k (S_{t+1}) \\given S_t = s, A_t = a\\right]\\\\\n",
        "= &\\max_a \\sum_{s^\\prime, r} p(s^\\prime, r \\given s, a)[r+ \\gamma v_k (s^\\prime)]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "After convergence, the optimal policy is derived from the state values, for all $s \\in \\states$:\n",
        "\n",
        "$$\n",
        "\\pi (s) = \\argmax_a \\sum_{s^\\prime, r} p(s^\\prime, r \\given s, a) [r + \\gamma v(s)]\n",
        "$$\n",
        "\n",
        "Pseudo code for value iteration from S&B p.65\n",
        "![value iteration](https://image.ibb.co/jLMfTx/Screen_Shot_2018_02_04_at_9_42_07_PM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib2W9eBomtgL",
        "colab_type": "text"
      },
      "source": [
        "# Implementation\n",
        "\n",
        "### Einstein Notation\n",
        "**Note on implementation**: The class `MDPAgent` implements two algorithms: policy iteration and value iteration. In both cases, the algorithms are implemented in matrix form, specifically using [Einstein summation convention](https://en.wikipedia.org/wiki/Einstein_notation) with the help of `numpy.einsum`. This notation is used to make the code more succinct and computationally efficient. If you are unfamiliar with Einstein notation, continue reading, otherwise skip to the section **MDP Agent**. Consider the following common operation. Given the 3-d array $A \\in \\R^{4 \\times 3 \\times 4}$, and vector $b \\in \\R^4$, we wish to compute the element-wise product of $A$ and $b$, with $b$ broadcast in the second and third dimension, followed by a summation over the second and third dimension resulting in vector $c \\in \\R^4$. This can be accomplished in `Numpy` the following way:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkceE2sSuNiU",
        "colab_type": "code",
        "outputId": "eac376e4-4b6b-4a0c-8af2-8c8552c4e90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.random.randint(0,5,(3,2,2))\n",
        "b = np.random.randint(0,5,3)\n",
        "\n",
        "c = a*b[:, None, None] # explicit broadcasting\n",
        "c = np.sum(c, axis=2) # sum 3rd dimension\n",
        "c = np.sum(c, axis=1) # sum 2nd\n",
        "\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10 32 24]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ALy3_PVvEsa",
        "colab_type": "text"
      },
      "source": [
        "The first issue is that the equation must be split into multiple lines. Secondly, Python creates temporary arrays, requiring more memory than necessary. We can compute the three lines above with a single function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPg5c07WvxOl",
        "colab_type": "code",
        "outputId": "59e86f7f-38fc-41a3-8868-47bafa472e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "c = np.einsum('ijk, i->i',a,b)\n",
        "\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[32 12  8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e5UDk5yfFX",
        "colab_type": "text"
      },
      "source": [
        "The operation is interpreted by the string argument. The first part, `ijk, i` labels the dimensions of the arrays `a` and `b` respectively. Array `b` has a single dimension, which is identified as `i`, meaning it matches the orientation of dimension `i` of array `a`. Since `b` is a vector, it must be multiplied by broadcasting to match the shape of `a`. The second part of the string, `->i`, tells `numpy` which dimension to return, and therefore how to sum along axes. If we write `->ij`, `numpy` returns a matrix matching the first two dimensions of `a`, and so must sum along dimension `k`. Since we wrote `->i`, `numpy` must sum along dimension `j` and `k`. Einstein notation can be used for dot-products or element-wise multiplication with broadcasting and summation, all in a single numpy function. See [`numpy.einsum`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.einsum.html) documentation for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9aCVYygwG8K",
        "colab_type": "text"
      },
      "source": [
        "## MDP Agent\n",
        "The `MDPAgent` class implements both policy iteration (class method `policy_iteration`) and value iteration (class method `value_iteration`). In the policy iteration case, the policy can be evaluated in closed form with a matrix inverse (class method `_policy_evaluation_exact`) or by iteration until convergence (class method `_policy_evaluation_modified`). Please read comments in the code for details of each function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc4D6dkheUQl",
        "colab_type": "code",
        "outputId": "a90a942d-517f-4fee-ce6b-5994a8e6e852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "from pdb import set_trace\n",
        "import numpy.ma as ma\n",
        "!pip install PTable\n",
        "from prettytable import PrettyTable, ALL\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def table_print(arr, headers=\"\",decimals=6):\n",
        "  x = PrettyTable()\n",
        "  x.field_names = headers\n",
        "  a = np.around(arr, decimals)\n",
        "  rows = a.shape[0]\n",
        "  if rows > 1 and a.ndim > 1:\n",
        "    for i in range(rows):\n",
        "      x.add_row([i, *a[i]])\n",
        "  else:\n",
        "    x.add_row(arr)\n",
        "  print(x)\n",
        "  \n",
        "def print_values(v,decimals):\n",
        "  table_print(np.expand_dims(v,1), headers=[\"states\", \"values\"],decimals=decimals)\n",
        "  \n",
        "def print_policy(pi, decimals):\n",
        "  headers = [\"states\"]\n",
        "  #set_trace()\n",
        "  for i in range(pi.shape[1]):\n",
        "     headers.append(\"action \" + str(i))\n",
        "  table_print(pi, headers=headers, decimals=decimals)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PTable\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/b3/b54301811173ca94119eb474634f120a49cd370f257d1aae5a4abaf12729/PTable-0.9.2.tar.gz\n",
            "Building wheels for collected packages: PTable\n",
            "  Running setup.py bdist_wheel for PTable ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/22/cc/2e/55980bfe86393df3e9896146a01f6802978d09d7ebcba5ea56\n",
            "Successfully built PTable\n",
            "Installing collected packages: PTable\n",
            "Successfully installed PTable-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlSCTCSzQ0M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MDPAgent():\n",
        "  def __init__(self, gamma):\n",
        "    \"\"\"\n",
        "    Basic MDP agent, can do policy iteration and value iteration\n",
        "    \"\"\"\n",
        "    self.gamma = gamma # discount rate\n",
        "    self.theta = 0.000001 # convergence threshold\n",
        "\n",
        "  def _initialize_deterministic(self, p, n_states, n_actions):\n",
        "    \"\"\" Initial policy \"\"\"\n",
        "    # Random deterministic policy, array of shape |S| x |A|\n",
        "    r_actions = np.random.randint(0,n_actions,n_states)\n",
        "    pi = np.zeros((r_actions.size, n_actions))\n",
        "    pi[np.arange(r_actions.size), r_actions] = 1\n",
        "    return pi\n",
        "\n",
        "  def _initialize_uniform(self, p, n_states, n_actions):\n",
        "    \"\"\" Initial policy \"\"\"\n",
        "    # Begin with uniform policy: array of shape |S| x |A|\n",
        "    pi = np.full((n_states, n_actions), 1)\n",
        "    pi = pi*np.sum(p, axis=2) # remove invalid actions\n",
        "    base = np.sum(pi, axis=1) # get number of valid actions per state\n",
        "    np.seterr(divide='ignore')\n",
        "    pi = np.nan_to_num(pi/base[:,None]) # divide by number of actions, broadcast\n",
        "    #np.seterr(divide='raise')\n",
        "    return pi\n",
        "\n",
        "  def _policy_evaluation_exact(self, pi, v, r, p):\n",
        "    \"\"\"\n",
        "    Evaluate policy by taking the inverse\n",
        "    Args:\n",
        "      pi: policy, array of shape |S| x |A|\n",
        "      r: rewards, array of shape |S| x |A|\n",
        "      p: state transition probabilities, array of shape |S| x |A| x |S|\n",
        "    Return:\n",
        "      v: 1D array with updated state values\n",
        "    \"\"\"\n",
        "    # Rewards according to policy: Hadamard product and row-wise sum\n",
        "    r_pi = np.einsum('ij,ij->i', pi, r)\n",
        "\n",
        "    # Policy-weighted transitions:\n",
        "    # multiply p by pi by broadcasting pi, then sum second axis\n",
        "    # result is an array of shape |S| x |S|\n",
        "    p_pi = np.einsum('ijk, ij->ik', p, pi)\n",
        "    v = np.dot(inv((np.eye(p_pi.shape[0]) - self.gamma*p_pi)), r_pi)\n",
        "    return v\n",
        "  \n",
        "  \n",
        "  def _policy_evaluation_modified(self, pi, v, r, p):\n",
        "    \"\"\"\n",
        "    Evaluate pi using an initial v estimate and iterate\n",
        "    Args:\n",
        "      pi: policy, array of shape |S| x |A|\n",
        "      v: state values, array of shape |S|\n",
        "      r: rewards, array of shape |S| x |A|\n",
        "      p: state transition probabilities, array of shape |S| x |A| x |S|\n",
        "    Return:\n",
        "      v: 1D array with updated state values\n",
        "    \"\"\"\n",
        "    max_iteration = 10000 # avoid core meltdown\n",
        "    for i in range(max_iteration):\n",
        "      # Rewards according to policy: Hadamard product and row-wise sum\n",
        "      r_pi = np.einsum('ij,ij->i', pi, r)\n",
        "\n",
        "      # Policy-weighted transitions:\n",
        "      # multiply p by pi by broadcasting pi, then sum second axis\n",
        "      # result is an array of shape |S| x |S|\n",
        "      p_pi = np.einsum('ijk, ij->ik', p, pi)\n",
        "\n",
        "      # New values\n",
        "      v_new = r_pi + self.gamma*np.dot(p_pi,v)\n",
        "      # Stop condition\n",
        "      if np.max(np.absolute(v - v_new)) < self.theta:\n",
        "        v = v_new\n",
        "        break;\n",
        "      v = v_new\n",
        "    return v\n",
        "\n",
        "  def _policy_improvement(self, v, r, p):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      v: state values, array of shape |S|\n",
        "      p: state transition probabilities, array of shape |S| x |A| x |S|\n",
        "    \"\"\"\n",
        "    # Get value for each action\n",
        "    #set_trace()\n",
        "    q = r + self.gamma*np.einsum('ijk, k->ij', p, v) # get q values\n",
        "    # If a 3rd dimension vector sums to 0, invalid action, so mask\n",
        "    q = ma.masked_array(q, mask=(np.sum(p,axis=2)-1)*(-1))\n",
        "    # New policy is max action (masked elements automatically ignored)\n",
        "    pi = (q == q.max(axis=1)[:,None]).astype(int)\n",
        "    pi = pi.filled(0)\n",
        "    # np.sum(p,axis=2)\n",
        "    # Break ties randomly\n",
        "    if pi.sum()>pi.shape[0]:\n",
        "      for i in range(pi.shape[0]):\n",
        "        if np.sum(pi[i]) == 0: continue\n",
        "        id = np.random.choice(np.where(pi[i] == pi[i].max())[0])\n",
        "        temp = np.zeros_like(pi[i])\n",
        "        temp[id] = 1\n",
        "        pi[i] = temp\n",
        "    return pi\n",
        "\n",
        "  def policy_iteration(self, r, p, method, initial='deterministic'):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      r: rewards, array of shape |S| x |A|\n",
        "      p: state transition probabilities, array of shape |S| x |A| x |S|\n",
        "      method: `exact` or `modified` (iterative)\n",
        "      initial: if `uniform`, policy is uniformly distributed at the start,\n",
        "        otherwise a deterministic policy is randomly generated\n",
        "    \"\"\"\n",
        "    t1 = datetime.now()\n",
        "    if method == 'exact':\n",
        "      policy_evaluation = self._policy_evaluation_exact\n",
        "    elif method =='modified':\n",
        "      policy_evaluation = self._policy_evaluation_modified\n",
        "    else:\n",
        "      raise ValueError(\"method must be 'exact' or 'modified'\")\n",
        "\n",
        "    n_states, n_actions = p.shape[:2]\n",
        "    # Initial policy estimates\n",
        "    if initial=='uniform':\n",
        "      pi = self._initialize_uniform(p, n_states, n_actions)\n",
        "    else:\n",
        "      pi = self._initialize_deterministic(p, n_states, n_actions)\n",
        "    v = np.zeros(n_states)\n",
        "\n",
        "    # Main loop\n",
        "    policy_stable = False\n",
        "    it = 0\n",
        "    while policy_stable == False:\n",
        "      v = policy_evaluation(pi, v, r, p)\n",
        "      old_actions = pi\n",
        "      pi = self._policy_improvement(v, r, p)\n",
        "      if np.array_equal(pi, old_actions): policy_stable = True\n",
        "      it += 1\n",
        "    \n",
        "    # Evaluate final policy\n",
        "    v = policy_evaluation(pi, v, r, p) \n",
        "    \n",
        "    t2 = datetime.now()\n",
        "    seconds = (t2 - t1).total_seconds()\n",
        "    return pi, v, it, seconds\n",
        "\n",
        "  def value_iteration(self, r, p):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      r: rewards, array of shape |S| x |A|\n",
        "      p: state transition probabilities, array of shape |S| x |A| x |S|\n",
        "    Return;\n",
        "      pi: policy, |S| x |A|\n",
        "      v: state values, |S|\n",
        "      it: number of iterations\n",
        "    \"\"\"\n",
        "    t1 = datetime.now()\n",
        "    n_states, n_actions = p.shape[:2]\n",
        "    v = np.zeros(n_states)\n",
        "    max_iteration = 10000\n",
        "    for it in range(max_iteration):\n",
        "      q = r + self.gamma*np.einsum('ijk, k->ij', p, v) # get q values\n",
        "      q = ma.masked_array(q, mask=(np.sum(p,axis=2)-1)*(-1)) #mask invalid actions\n",
        "      v_new = np.max(q, axis=1) # state-values equal max possible values\n",
        "      v_new = v_new.filled(0) # Masked states should have value 0\n",
        "      if np.max(np.absolute(v - v_new)) < self.theta:\n",
        "        v = v_new\n",
        "        break;\n",
        "      v = v_new\n",
        "    # Derive new policy\n",
        "    pi = (q == q.max(axis=1)[:,None]).astype(int)\n",
        "    pi = pi.filled(0)\n",
        "    \n",
        "    t2 = datetime.now()\n",
        "    seconds = (t2 - t1).total_seconds()\n",
        "    return pi, v, it+1, seconds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tKXWSLGUDUY",
        "colab_type": "code",
        "outputId": "520c7e9b-996d-4622-9504-7f0ded0e74c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "def policy_eval(self, pi, v, r, p):\n",
        "    \"\"\"\n",
        "    Evaluate policy by taking the inverse\n",
        "    Args:\n",
        "      pi: policy, array of shape |S| x |A|\n",
        "      r: rewards, array of shape |S| x |A|\n",
        "      p: state transition probabilities, array of shape |S| x |A| x |S|\n",
        "    Return:\n",
        "      v: 1D array with updated state values\n",
        "    \"\"\"\n",
        "    # Rewards according to policy: Hadamard product and row-wise sum\n",
        "    r_pi = np.einsum('ij,ij->i', pi, r)\n",
        "\n",
        "    # Policy-weighted transitions:\n",
        "    # multiply p by pi by broadcasting pi, then sum second axis\n",
        "    # result is an array of shape |S| x |S|\n",
        "    p_pi = np.einsum('ijk, ij->ik', p, pi)\n",
        "    v = np.dot(inv((np.eye(p_pi.shape[0]) - self.gamma*p_pi)), r_pi)\n",
        "    return v\n",
        "\n",
        "gamma = 0.5\n",
        "policy = np.array(\n",
        "      [[1,0],\n",
        "      [0,1],\n",
        "      [1,0]])\n",
        "\n",
        "p_pi = np.array(\n",
        "        [[0.8, 0.4, 0],\n",
        "        [0.15, 0.5, 0],\n",
        "        [0.05, 0.1, 1]])\n",
        "\n",
        "r_pi = np.array(\n",
        "         [[2,1,0]])\n",
        "\n",
        "v = np.dot(inv((np.eye(p_pi.shape[0]) - gamma*p_pi)), r_pi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-36244b5b0001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m          [[2,1,0]])\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: shapes (3,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ADobcx3P7Wc",
        "colab_type": "text"
      },
      "source": [
        "## Basic MDP Experiment\n",
        "\n",
        "The first MDP is a basic MDP which consists of two states and three actions, with the following transition probabilities and rewards:\n",
        "\n",
        "**Transition Probabilities**:\n",
        "- P(s_0 | s_0, a_0) = 0.5\n",
        "- P(s_1 | s_0, a_0) = 0.5\n",
        "- P(s_0 | s_0, a_1) = 0\n",
        "- P(s_1 | s_0, a_1) = 1\n",
        "- P(s_1 | s_0, a_2) = 0\n",
        "- P(s_1 | s_1, a_2) = 1\n",
        "\n",
        "**Rewards**:\n",
        "- r(s_0, a_0) = 5\n",
        "- r(s_0, a_1) = 10\n",
        "- r(s_1, a_2) = -1\n",
        "\n",
        "The above MDP is implemented below as a class environment with transitions as array `p` and rewards as array `r`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlM8WuFNm6zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicMDP():\n",
        "  def __init__(self):\n",
        "    \"\"\"Very basic MDP to test policy/value iteration\"\"\"\n",
        "    # Transition probabilities is a 3D array of shape |S| x |A| x |S|\n",
        "    # the \n",
        "    self.p = np.array(\n",
        "        [[[0.5, 0.5],[0,1],[0,0]],\n",
        "         [[0.0, 0.0],[0,0],[0,1]]])\n",
        "\n",
        "    # Rewards is a function of state and action, i.e. r(s,a), shape |S| x |A|\n",
        "    self.r = np.array(\n",
        "        [[5, 10,  0],\n",
        "         [0,  0, -1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TogFyUfA87l",
        "colab_type": "text"
      },
      "source": [
        "Let's solve this MDP and see if we arrive at the same solution using policy iteration, modified policy iteration, value iteration, and their compute time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI3SpukP8s2H",
        "colab_type": "code",
        "outputId": "2a857f5e-b5a7-4e6f-ea62-1f97fe9071bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1019
        }
      },
      "source": [
        "def print_info(pi, v, it, sec, title):\n",
        "  print(\"=\"*79)\n",
        "  print(title)\n",
        "  print(\"Final Policy:\")\n",
        "  print_policy(pi,2)\n",
        "  print(\"Values:\")\n",
        "  print_values(v,6)\n",
        "  print(\"compute seconds: \", sec)\n",
        "  print(\"iterations: \", it)\n",
        "  print()\n",
        "\n",
        "def experiment_1():\n",
        "  gamma = 0.95\n",
        "  agent = MDPAgent(gamma)\n",
        "  env = BasicMDP()\n",
        "  \n",
        "  title = \"Policy Iteration with closed form policy evaluation\"\n",
        "  pi1, v, it, sec = agent.policy_iteration(env.r, env.p, method='exact', initial='deterministic')\n",
        "  print_info(pi1,v,it,sec,title)\n",
        "\n",
        "  title = \"Modified Policy Iteration with partial policy evaluation\"\n",
        "  pi2, v, it, sec = agent.policy_iteration(env.r, env.p, method='modified', initial='deterministic')\n",
        "  print_info(pi2,v,it,sec,title)\n",
        "\n",
        "  title = \"Value Iteration\"\n",
        "  pi3, v, it, sec = agent.value_iteration(env.r, env.p)\n",
        "  print_info(pi3,v,it,sec,title)\n",
        "  \n",
        "  if (np.array_equal(pi1,pi2) and np.array_equal(pi2, pi3)):\n",
        "    print(\"all policies are equal\")\n",
        "  else:\n",
        "    print(\"policies not equal\")\n",
        "  \n",
        "experiment_1()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===============================================================================\n",
            "Policy Iteration with closed form policy evaluation\n",
            "Final Policy:\n",
            "+--------+----------+----------+----------+\n",
            "| states | action 0 | action 1 | action 2 |\n",
            "+--------+----------+----------+----------+\n",
            "|   0    |    1     |    0     |    0     |\n",
            "|   1    |    0     |    0     |    1     |\n",
            "+--------+----------+----------+----------+\n",
            "Values:\n",
            "+--------+-----------+\n",
            "| states |   values  |\n",
            "+--------+-----------+\n",
            "|   0    | -8.571429 |\n",
            "|   1    |   -20.0   |\n",
            "+--------+-----------+\n",
            "compute seconds:  0.003935\n",
            "iterations:  3\n",
            "\n",
            "===============================================================================\n",
            "Modified Policy Iteration with partial policy evaluation\n",
            "Final Policy:\n",
            "+--------+----------+----------+----------+\n",
            "| states | action 0 | action 1 | action 2 |\n",
            "+--------+----------+----------+----------+\n",
            "|   0    |    1     |    0     |    0     |\n",
            "|   1    |    0     |    0     |    1     |\n",
            "+--------+----------+----------+----------+\n",
            "Values:\n",
            "+--------+------------+\n",
            "| states |   values   |\n",
            "+--------+------------+\n",
            "|   0    | -8.571411  |\n",
            "|   1    | -19.999983 |\n",
            "+--------+------------+\n",
            "compute seconds:  0.035854\n",
            "iterations:  1\n",
            "\n",
            "===============================================================================\n",
            "Value Iteration\n",
            "Final Policy:\n",
            "+--------+----------+----------+----------+\n",
            "| states | action 0 | action 1 | action 2 |\n",
            "+--------+----------+----------+----------+\n",
            "|   0    |    1     |    0     |    0     |\n",
            "|   1    |    0     |    0     |    1     |\n",
            "+--------+----------+----------+----------+\n",
            "Values:\n",
            "+--------+------------+\n",
            "| states |   values   |\n",
            "+--------+------------+\n",
            "|   0    |  -8.57141  |\n",
            "|   1    | -19.999982 |\n",
            "+--------+------------+\n",
            "compute seconds:  0.090094\n",
            "iterations:  271\n",
            "\n",
            "all policies are equal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yH588S6jqG8",
        "colab_type": "text"
      },
      "source": [
        "All 3 methods arrive at the same policy and values. It seems, however, that on this trivial MDP the policy iteration with the closed form version of policy evaluation is the fastest method, while only marginally faster than modified policy iteration. This may be due to the random initialization of our policy, and so it would be prudent to repeat the exercise and average the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JlAa8xrkxtS",
        "colab_type": "code",
        "outputId": "92fb318f-bfd5-42b2-d88c-9565c25c4ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "def experiment_2():\n",
        "  gamma = 0.95\n",
        "  agent = MDPAgent(gamma)\n",
        "  env = BasicMDP()\n",
        "  \n",
        "  trials = 100\n",
        "  pol_it = np.zeros(trials)\n",
        "  mod_it = np.zeros(trials)\n",
        "  val_it = np.zeros(trials)\n",
        "  pol_time = np.zeros(trials)\n",
        "  mod_time = np.zeros(trials)\n",
        "  val_time = np.zeros(trials)\n",
        "\n",
        "  for i in range(trials):\n",
        "    _, _, pol_it[i], pol_time[i] = agent.policy_iteration(env.r, env.p, method='exact', initial='deterministic')\n",
        "    _, _, mod_it[i], mod_time[i] = agent.policy_iteration(env.r, env.p, method='modified', initial='deterministic')\n",
        "    _, _, val_it[i], val_time[i] = agent.value_iteration(env.r, env.p)\n",
        "\n",
        "  print(\"Policy iteration: {:.4f} seconds, {:.2f} iterations\".format(np.sum(pol_time)/trials, np.sum(pol_it)/trials))\n",
        "  print(\"Modified policy : {:.4f} seconds, {:.2f} iterations\".format(np.sum(mod_time)/trials, np.sum(mod_it)/trials))\n",
        "  print(\"Value iteration : {:.4f} seconds, {:.2f} iterations\".format(np.sum(val_time)/trials, np.sum(val_it)/trials))\n",
        "\n",
        "experiment_2()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy iteration: 0.0022 seconds, 2.69 iterations\n",
            "Modified policy : 0.0392 seconds, 2.52 iterations\n",
            "Value iteration : 0.0807 seconds, 271.00 iterations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9H-mfU4slb8",
        "colab_type": "text"
      },
      "source": [
        "Value iteration may seem more elegant mathematically, since it does not alternate between policy evaluation and policy improvement, it takes much longer to converge on this MDP as it requires a large number of iterations to converge. Policy iteration converges much faster than other methods, although this might not be feasible in a realistic MDP where taking a matrix inverse may not be feasible. Next, we test these algorithms in a Grid World setup. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhwwIY_N_o85",
        "colab_type": "text"
      },
      "source": [
        "## Grid World Experiment\n",
        "\n",
        "To experiment with a more elaborate MDP, the GridWorld class is implemented below. The class will automatically create transition and reward arrays for an arbitrary sized grid and any number of terminal states. Rewards are -1 for all transitions, while terminal states have 0 reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTd47cWlamwo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95gFQOcnmuI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GridWorld():\n",
        "  def __init__(self, rows, cols, terminal_states, reward_value):\n",
        "    \"\"\"\n",
        "    Creates a GridWorld of size `rows` X `cols` with the listed terminal states\n",
        "    Args:\n",
        "      rows: int\n",
        "      cols: int\n",
        "      terminal_states: list of int where each element is the grid index of the\n",
        "        terminal state. The grid's cells are numbered left to right, top to\n",
        "        bottom, starting with index 0.\n",
        "    \"\"\"\n",
        "    self.rows, self.cols = rows, cols\n",
        "    self.terminal_states = terminal_states\n",
        "    self.n_states = rows * cols # number of states\n",
        "    self.n_actions = 4 # left, up, right, down\n",
        "    self.p = self._init_dynamics(rows, cols, self.n_states, self.n_actions, terminal_states)\n",
        "    self.r = self._init_rewards(self.n_states, self.n_actions, reward_value, terminal_states)\n",
        "\n",
        "  def _init_dynamics(self, rows, cols, n_states, n_actions, terminal_states):\n",
        "    \"\"\" Returns model dynamics array of shape |S| x |A| x |S| \"\"\"\n",
        "    # Empty transition array\n",
        "    p = np.zeros((n_states,n_actions,n_states))\n",
        "    # Add deterministic transitions by traversing an imaginary grid\n",
        "    for r in range(rows):\n",
        "      for c in range(cols):\n",
        "        cur_s = rows*r + c # current state\n",
        "\n",
        "        # left?\n",
        "        left_s = rows*r + c -1 # left state id\n",
        "        if (c-1) >= 0: p[cur_s,0, left_s] = 1\n",
        "\n",
        "        # up?\n",
        "        up_s = rows*(r - 1) + c # up state id\n",
        "        if (r-1) >= 0: p[cur_s, 1, up_s] = 1;\n",
        "\n",
        "        # right?\n",
        "        right_s = rows*r + c + 1 # right state id\n",
        "        if (c+1) < cols: p[cur_s, 2, right_s] = 1;\n",
        "\n",
        "        # down?\n",
        "        down_s = rows*(r + 1) + c # down state id\n",
        "        if (r+1) < rows: p[cur_s, 3, down_s] = 1;\n",
        "\n",
        "    # Terminal states have no actions\n",
        "    for t in terminal_states:\n",
        "      p[t] = 0\n",
        "    return p\n",
        "\n",
        "  def _init_rewards(self, n_states, n_actions, reward_value, terminal_states):\n",
        "    \"\"\" Returns reward matrix, shape |S| x |A| \"\"\"\n",
        "    # Rewards are -1 on all actions. No rewards for terminal since no action\n",
        "    r = np.full((n_states, n_actions), reward_value)\n",
        "    for t in terminal_states:\n",
        "      r[t] = 0\n",
        "    return r\n",
        "  \n",
        "  def render_policy(self, pi):\n",
        "    pass\n",
        "\n",
        "  def __str__(self):\n",
        "    \"\"\" Print the grid \"\"\"\n",
        "    x = PrettyTable()\n",
        "    x.header = False\n",
        "\n",
        "    for r in range(self.rows):\n",
        "      row = []\n",
        "      for c in range(self.cols):\n",
        "        cur_s = self.rows*r + c # current state\n",
        "        if cur_s in self.terminal_states:\n",
        "          row.append('#')\n",
        "        else:\n",
        "          row.append('-')\n",
        "      x.add_row(row)\n",
        "    x.hrules = ALL\n",
        "    return x.get_string()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsDziouMxl9q",
        "colab_type": "text"
      },
      "source": [
        "We first start with a simple 4 x 4 grid with 3 terminal states and visualize it. Note the hash symbol stands for terminal states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye8A4gb6RenU",
        "colab_type": "code",
        "outputId": "01c1df00-290f-4141-a21d-ff250a37150c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "rows = cols = 4\n",
        "terminal_states = [0,6,15]\n",
        "env = GridWorld(rows, cols, terminal_states, reward_value=-1)\n",
        "print(env)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+---+\n",
            "| # | - | - | - |\n",
            "+---+---+---+---+\n",
            "| - | - | # | - |\n",
            "+---+---+---+---+\n",
            "| - | - | - | - |\n",
            "+---+---+---+---+\n",
            "| - | - | - | # |\n",
            "+---+---+---+---+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUNzGAKfyjEy",
        "colab_type": "text"
      },
      "source": [
        "### Baseline\n",
        "\n",
        "Let's first test if all methods arrive at the same policy, and their compute time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p6XZhf_uKiR",
        "colab_type": "code",
        "outputId": "954eb4a0-5996-496a-fddc-013e8c23ea04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "def experiment_3():\n",
        "  rows = cols = 4\n",
        "  terminal_states = [0,6,15]\n",
        "  env = GridWorld(rows, cols, terminal_states, reward_value=-1)\n",
        "  gamma = 0.95\n",
        "  agent = MDPAgent(gamma)\n",
        "  \n",
        "  title = \"Policy Iteration with closed form policy evaluation\"\n",
        "  pi1, v, it, sec = agent.policy_iteration(env.r, env.p, method='exact', initial='deterministic')\n",
        "  print(title)\n",
        "  print(sec)\n",
        "\n",
        "  title = \"Modified Policy Iteration with partial policy evaluation\"\n",
        "  pi2, v, it, sec = agent.policy_iteration(env.r, env.p, method='modified', initial='deterministic')\n",
        "  print(title)\n",
        "  print(sec)\n",
        "\n",
        "  title = \"Value Iteration\"\n",
        "  pi3, v, it, sec = agent.value_iteration(env.r, env.p)\n",
        "  print(title)\n",
        "  print(sec)\n",
        "  \n",
        "  if (np.array_equal(pi1,pi2) and np.array_equal(pi2, pi3)):\n",
        "    print(\"all policies are equal\")\n",
        "  else:\n",
        "    print(\"policies not equal\")\n",
        "  \n",
        "experiment_3()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy Iteration with closed form policy evaluation\n",
            "0.005941\n",
            "Modified Policy Iteration with partial policy evaluation\n",
            "0.039605\n",
            "Value Iteration\n",
            "0.001773\n",
            "all policies are equal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF543XaqJAl0",
        "colab_type": "text"
      },
      "source": [
        "### Varying discount rate\n",
        "\n",
        "They all converge once again to the same solution. If we vary gamma, how does this affect convergence time?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W1oI0h-fEEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_plot_data(x, data, names, xlabel):\n",
        "  \"\"\" data, names are lists of vectors \"\"\"\n",
        "  for i, y in enumerate(data):\n",
        "    plt.plot(x, y, 'o', markersize=3, label=names[i])\n",
        "  plt.legend(loc='upper left', prop={'size': 16}, numpoints=10)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(\"milliseconds\")\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIUiYT85Mz3u",
        "colab_type": "code",
        "outputId": "c2c13ffa-7550-4348-ab3d-7fd516c9d897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "def experiment_4():\n",
        "  rows = cols = 4\n",
        "  terminal_states = [0,6,15]\n",
        "  env = GridWorld(rows, cols, terminal_states, reward_value=-1)\n",
        "  trials = 10\n",
        "  samples = 200\n",
        "  gammas = np.linspace(0.001,1,endpoint=False,num=samples)\n",
        "  pol_time = np.zeros((trials, samples))\n",
        "  mod_time = np.zeros((trials, samples))\n",
        "  val_time = np.zeros((trials, samples))\n",
        "  \n",
        "  for t in range(trials):\n",
        "    for i in range(samples):\n",
        "      agent = MDPAgent(gammas[i])\n",
        "      _, _, _, pol_time[t,i] = agent.policy_iteration(env.r, env.p, method='exact', initial='uniform')\n",
        "      _, _, _, mod_time[t,i] = agent.policy_iteration(env.r, env.p, method='modified', initial='uniform')\n",
        "      _, _, _, val_time[t,i] = agent.value_iteration(env.r, env.p)\n",
        "  \n",
        "  # Plot data\n",
        "  pol_time = np.average(pol_time, axis=0)*1000\n",
        "  mod_time = np.average(mod_time, axis=0)*1000\n",
        "  val_time = np.average(val_time, axis=0)*1000\n",
        "  data = [pol_time, mod_time, val_time]\n",
        "  names = [\"Policy Iteration\", \"Modified PI\", \"Value Iteration\"]\n",
        "  multi_plot_data(gammas, data, names, xlabel=\"gamma\")\n",
        "\n",
        "experiment_4()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFYCAYAAAB+s6Q9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niYiIHMT/AwqbCa4jy0jXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1614bca630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1l2MAEaYOUT",
        "colab_type": "text"
      },
      "source": [
        "In this setup, value iteration computes faster than either policy iteration or modified policy iteration, with modified policy iteration increasing exponentially with gamma. Value iteration is quite robust varying gamma.\n",
        "\n",
        "### Varying MDP Complexity\n",
        "Let's see if increasing the model complexity has an large impact on the algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n7xZVK6Z-hj",
        "colab_type": "code",
        "outputId": "5b9d8eff-99a2-49ec-f5ab-2b87aab69503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "def experiment_5():\n",
        "  gamma = 0.95\n",
        "  agent = MDPAgent(gamma)\n",
        "  terminal_states = [0,6]\n",
        "  rows = cols = 5\n",
        "  env = GridWorld(rows, cols, terminal_states, reward_value=-1)\n",
        "  \n",
        "  _, _, pol_it, pol_time = agent.policy_iteration(env.r, env.p, method='exact', initial='deterministic')\n",
        "  _, _, mod_it, mod_time = agent.policy_iteration(env.r, env.p, method='modified', initial='deterministic')\n",
        "  _, _, val_it, val_time = agent.value_iteration(env.r, env.p)\n",
        "\n",
        "  print(\"Policy iteration: {:.4f} seconds, {:.2f} iterations\".format(pol_time, pol_it))\n",
        "  print(\"Modified policy : {:.4f} seconds, {:.2f} iterations\".format(mod_time, mod_it))\n",
        "  print(\"Value iteration : {:.4f} seconds, {:.2f} iterations\".format(val_time, val_it))\n",
        "\n",
        "experiment_5()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy iteration: 92.0373 seconds, 46441.00 iterations\n",
            "Modified policy : 307.7097 seconds, 216967.00 iterations\n",
            "Value iteration : 0.0024 seconds, 7.00 iterations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO9i4qXOUX0u",
        "colab_type": "text"
      },
      "source": [
        "Increasing the grid size by a single column and row makes computation time explode for Policy Iteration and much worse for Modified Policy Iteration. Quite surprisingly, it's no sweat for Value Iteration. Value Iteration converges a fraction of a second, compared to 307 in the case of modified policy iteration. It only required 7 iterations compared to 216,967 for modified policy. Outstanding!\n",
        "\n"
      ]
    }
  ]
}